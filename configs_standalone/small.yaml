# Small model configuration for testing and debugging
# This uses smaller dimensions to run faster on limited hardware.

model:
  n_layer: 4
  d_model: 512
  n_ctx: 256
  d_head: 16
  d_mlp: null  # 2048
  
  use_rms_norm: true
  tie_embeddings: false
  use_positional_embeddings: false
  use_bigram_table: true
  use_attention_sinks: true
  activation: gelu
  dropout: 0.0
  use_bias: true
  use_flash_attention: true

sparsity:
  enable_weight_sparsity: true
  target_l0_fraction: 0.1  # Less sparse for faster testing
  sparsity_anneal_start_fraction: 0.01
  sparsity_anneal_end_fraction: 0.5
  min_weights_per_neuron: 4
  
  enable_activation_sparsity: true
  activation_topk_fraction: 0.25
  activation_sparsity_locations: "attn_in,attn_out,mlp_in,mlp_out,mlp_neuron,attn_v,attn_k,attn_q"

optimizer:
  learning_rate: 0.0128
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.1
  eps: 0.1
  enable_grad_clip: true
  grad_clip_rms: 1.0
  warmup_fraction: 0.01
  enable_lr_decay: true
  use_sharkfin_schedule: false

training:
  dataset_name: ""  # Set this
  dataset_split: train
  text_column: text
  tokenizer_name: ""  # Set this
  
  total_tokens: 10000000  # 10M tokens for testing
  batch_size: 32
  gradient_accumulation_steps: 1
  mixed_precision: bf16
  
  checkpoint_dir: checkpoints
  checkpoint_every_n_steps: 500
  keep_n_checkpoints: 3
  
  log_every_n_steps: 1
  log_gradients_every_n_steps: 10
  log_weights_every_n_steps: 100
  log_sparsity_every_n_steps: 50
  eval_every_n_steps: 20
  
  # Validation - uses "test" split if available, else holds out from train
  val_split: test
  val_holdout_fraction: 0.01
  val_max_batches: 20
  
  wandb_project: circuit_sparsity
  wandb_run_name: null
  wandb_entity: null
  use_wandb: true
  
  seed: 0
  
  # HuggingFace Hub - set to upload final checkpoint (e.g., "username/model-name")
  hf_repo: null

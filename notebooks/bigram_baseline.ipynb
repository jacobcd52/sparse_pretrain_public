{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f55c43b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context length: 512\n",
      "Dataset: SimpleStories/SimpleStories\n",
      "Tokenizer: SimpleStories/SimpleStories-1.25M\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bigram Baseline for SimpleStories Dataset\n",
    "==========================================\n",
    "A bigram model predicts the next token based only on the current token.\n",
    "This serves as a simple baseline for language modeling.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CONTEXT_LENGTH = 512\n",
    "DATASET_NAME = \"SimpleStories/SimpleStories\"\n",
    "TOKENIZER_NAME = \"SimpleStories/SimpleStories-1.25M\"\n",
    "\n",
    "print(f\"Context length: {CONTEXT_LENGTH}\")\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Tokenizer: {TOKENIZER_NAME}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4eba6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4096\n",
      "EOS token ID: 1\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"EOS token ID: {eos_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97cc15ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 2115696\n",
      "Test examples: 21371\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset_fast(dataset, tokenizer, text_column=\"story\", batch_size=1000):\n",
    "    \"\"\"Tokenize all texts using batch processing for speed.\"\"\"\n",
    "    all_tokens = []\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Process in batches\n",
    "    texts = dataset[text_column]\n",
    "    total = len(texts)\n",
    "    \n",
    "    for i in tqdm(range(0, total, batch_size), desc=\"Tokenizing batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        # Batch encode\n",
    "        encoded = tokenizer(batch_texts, add_special_tokens=False)\n",
    "        for token_ids in encoded[\"input_ids\"]:\n",
    "            all_tokens.extend(token_ids)\n",
    "            if eos_id is not None:\n",
    "                all_tokens.append(eos_id)\n",
    "    \n",
    "    return all_tokens\n",
    "\n",
    "def create_chunks(tokens, context_length):\n",
    "    \"\"\"Split tokens into fixed-length chunks.\"\"\"\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens) - context_length, context_length):\n",
    "        chunks.append(tokens[i:i + context_length])\n",
    "    return chunks\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "test_dataset = load_dataset(DATASET_NAME, split=\"test\")\n",
    "\n",
    "print(f\"Train examples: {len(train_dataset)}\")\n",
    "print(f\"Test examples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3aca41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing batches: 100%|██████████| 2116/2116 [05:07<00:00,  6.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: 608,617,592\n",
      "\n",
      "Tokenizing test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing batches: 100%|██████████| 22/22 [00:04<00:00,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test tokens: 6,148,046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize train and test sets (using fast batch processing)\n",
    "print(\"Tokenizing train set...\")\n",
    "train_tokens = tokenize_dataset_fast(train_dataset, tokenizer)\n",
    "print(f\"Train tokens: {len(train_tokens):,}\")\n",
    "\n",
    "print(\"\\nTokenizing test set...\")\n",
    "test_tokens = tokenize_dataset_fast(test_dataset, tokenizer)\n",
    "print(f\"Test tokens: {len(test_tokens):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "456ea13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building bigram statistics from training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting bigrams: 100%|██████████| 608617591/608617591 [04:17<00:00, 2367447.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens seen: 4018\n",
      "Total bigrams counted: 608,617,591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build bigram model from training data\n",
    "print(\"Building bigram statistics from training data...\")\n",
    "\n",
    "# Count bigrams: bigram_counts[prev_token][next_token] = count\n",
    "bigram_counts = {}\n",
    "unigram_counts = Counter()\n",
    "\n",
    "for i in tqdm(range(len(train_tokens) - 1), desc=\"Counting bigrams\"):\n",
    "    prev_token = train_tokens[i]\n",
    "    next_token = train_tokens[i + 1]\n",
    "    \n",
    "    unigram_counts[prev_token] += 1\n",
    "    \n",
    "    if prev_token not in bigram_counts:\n",
    "        bigram_counts[prev_token] = Counter()\n",
    "    bigram_counts[prev_token][next_token] += 1\n",
    "\n",
    "# Also count the last token for unigrams\n",
    "unigram_counts[train_tokens[-1]] += 1\n",
    "\n",
    "print(f\"Unique tokens seen: {len(unigram_counts)}\")\n",
    "print(f\"Total bigrams counted: {sum(sum(c.values()) for c in bigram_counts.values()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c722bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting counts to log probabilities with add-1 (Laplace) smoothing...\n",
      "Smoothing alpha: 1\n",
      "Vocab size for smoothing: 4096\n"
     ]
    }
   ],
   "source": [
    "# Convert counts to log probabilities with add-1 smoothing\n",
    "# P(next_token | prev_token) = (count(prev, next) + 1) / (count(prev) + vocab_size)\n",
    "\n",
    "print(\"Converting counts to log probabilities with add-1 (Laplace) smoothing...\")\n",
    "\n",
    "# For efficiency, we'll compute log probs on the fly during evaluation\n",
    "# But let's precompute the denominators\n",
    "smoothed_denominators = {}\n",
    "for token in unigram_counts:\n",
    "    smoothed_denominators[token] = unigram_counts[token] + vocab_size\n",
    "\n",
    "print(f\"Smoothing alpha: 1\")\n",
    "print(f\"Vocab size for smoothing: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d988bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating bigram model on test set...\n",
      "Test tokens: 6,148,046\n",
      "Test chunks (context_length=512): 12007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12007/12007 [00:04<00:00, 2403.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Results on test set:\n",
      "==================================================\n",
      "Total predictions: 6,135,577\n",
      "Unseen previous tokens: 0 (0.00%)\n",
      "Cross-Entropy Loss: 3.8769\n",
      "Perplexity: 48.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "# Cross-entropy loss = -1/N * sum(log P(next_token | prev_token))\n",
    "\n",
    "print(\"Evaluating bigram model on test set...\")\n",
    "print(f\"Test tokens: {len(test_tokens):,}\")\n",
    "\n",
    "# Create test chunks of context_length\n",
    "test_chunks = create_chunks(test_tokens, CONTEXT_LENGTH)\n",
    "print(f\"Test chunks (context_length={CONTEXT_LENGTH}): {len(test_chunks)}\")\n",
    "\n",
    "total_log_prob = 0.0\n",
    "total_predictions = 0\n",
    "unseen_prev_tokens = 0\n",
    "\n",
    "for chunk in tqdm(test_chunks, desc=\"Evaluating\"):\n",
    "    for i in range(len(chunk) - 1):\n",
    "        prev_token = chunk[i]\n",
    "        next_token = chunk[i + 1]\n",
    "        \n",
    "        # Get bigram probability with smoothing\n",
    "        if prev_token in bigram_counts:\n",
    "            count = bigram_counts[prev_token].get(next_token, 0) + 1  # +1 for smoothing\n",
    "            denom = smoothed_denominators[prev_token]\n",
    "        else:\n",
    "            # Unseen prev_token - use uniform distribution\n",
    "            count = 1\n",
    "            denom = vocab_size\n",
    "            unseen_prev_tokens += 1\n",
    "        \n",
    "        prob = count / denom\n",
    "        total_log_prob += np.log(prob)\n",
    "        total_predictions += 1\n",
    "\n",
    "# Calculate cross-entropy loss (negative log likelihood)\n",
    "ce_loss = -total_log_prob / total_predictions\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Results on test set:\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total predictions: {total_predictions:,}\")\n",
    "print(f\"Unseen previous tokens: {unseen_prev_tokens:,} ({100*unseen_prev_tokens/total_predictions:.2f}%)\")\n",
    "print(f\"Cross-Entropy Loss: {ce_loss:.4f}\")\n",
    "print(f\"Perplexity: {np.exp(ce_loss):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed180d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

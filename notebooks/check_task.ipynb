{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26cdc23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: jacobcd52/ss_bridges_d1024_f0.015625\n",
      "Loading sparse model from HuggingFace Hub bridge checkpoint: jacobcd52/ss_bridges_d1024_f0.015625\n",
      "Note: Bridge weights are ignored for pruning\n",
      "Model loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sparse_pretrain.src.pruning.run_pruning import load_model\n",
    "\n",
    "# Load model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"jacobcd52/ss_bridges_d1024_f0.015625\"\n",
    "tokenizer_name = \"SimpleStories/SimpleStories-1.25M\"\n",
    "\n",
    "print(f\"Loading model: {model_path}\")\n",
    "model, _ = load_model(model_path, device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "print(f\"Model loaded on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd59d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'when jose wrote a letter, he'\n",
      "Token IDs: [335, 730, 1050, 32, 776, 13, 103]\n",
      "Tokens: ['when', 'jose', 'wrote', 'a', 'letter', ',', 'he']\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# EDIT YOUR PROMPT HERE\n",
    "# ========================================\n",
    "prompt = 'when jose wrote a letter, he'\n",
    "\n",
    "# Tokenize\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Token IDs: {input_ids[0].tolist()}\")\n",
    "print(f\"Tokens: {[tokenizer.decode([t]) for t in input_ids[0].tolist()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f348fdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 7, 4096])\n",
      "Final position logits shape: torch.Size([4096])\n",
      "\n",
      "Top 10 next token predictions:\n",
      "----------------------------------------\n",
      "1. '\"' (id=3) - prob=0.6352\n",
      "2. 'he' (id=103) - prob=0.0656\n",
      "3. 'saying' (id=1929) - prob=0.0283\n",
      "4. 'telling' (id=1868) - prob=0.0162\n",
      "5. 'asking' (id=2393) - prob=0.0147\n",
      "6. 'and' (id=94) - prob=0.0117\n",
      "7. 'a' (id=32) - prob=0.0102\n",
      "8. 'kim' (id=415) - prob=0.0102\n",
      "9. 'the' (id=85) - prob=0.0066\n",
      "10. 'it' (id=120) - prob=0.0056\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    logits = output[0] if isinstance(output, tuple) else output\n",
    "\n",
    "# Get logits at final position\n",
    "final_logits = logits[0, -2, :]  # (vocab_size,)\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Final position logits shape: {final_logits.shape}\")\n",
    "\n",
    "# Get top 10 predictions\n",
    "probs = torch.softmax(final_logits, dim=-1)\n",
    "top_probs, top_indices = torch.topk(probs, 10)\n",
    "\n",
    "print(f\"\\nTop 10 next token predictions:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "    token = tokenizer.decode([idx.item()])\n",
    "    print(f\"{i+1}. '{token}' (id={idx.item()}) - prob={prob.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e347a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6873203",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7077a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc801827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: dummy_pronoun_val\n",
      "Templates: 24\n",
      "\n",
      "================================================================================\n",
      "Showing 30 examples from dummy pronoun task\n",
      "================================================================================\n",
      "\n",
      "Example 1:\n",
      "  Prompt: \"when rita helped the lost puppy,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.319 ✓\n",
      "    2. 'leaving' - 0.065  \n",
      "    3. 'the' - 0.051  \n",
      "  Logit diff (correct - incorrect): 2.64 ✓\n",
      "\n",
      "Example 2:\n",
      "  Prompt: \"when mia fed the hungry cat,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.213 ✓\n",
      "    2. 'the' - 0.052  \n",
      "    3. 'it' - 0.028  \n",
      "  Logit diff (correct - incorrect): 2.28 ✓\n",
      "\n",
      "Example 3:\n",
      "  Prompt: \"when leo cheered for the team,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.369 ✓\n",
      "    2. 'they' - 0.086  \n",
      "    3. 'knowing' - 0.041  \n",
      "  Logit diff (correct - incorrect): 3.52 ✓\n",
      "\n",
      "Example 4:\n",
      "  Prompt: \"when maria swam in the lake,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.528 ✓\n",
      "    2. 'the' - 0.081  \n",
      "    3. 'they' - 0.057  \n",
      "  Logit diff (correct - incorrect): 3.25 ✓\n",
      "\n",
      "Example 5:\n",
      "  Prompt: \"when peter collected pretty shells,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.174 ✓\n",
      "    2. 'they' - 0.079  \n",
      "    3. 'and' - 0.046  \n",
      "  Logit diff (correct - incorrect): 3.08 ✓\n",
      "\n",
      "Example 6:\n",
      "  Prompt: \"when rita won the spelling bee,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.246 ✓\n",
      "    2. 'it' - 0.087  \n",
      "    3. 'the' - 0.087  \n",
      "  Logit diff (correct - incorrect): 2.83 ✓\n",
      "\n",
      "Example 7:\n",
      "  Prompt: \"when samuel swam in the lake,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.599 ✓\n",
      "    2. 'the' - 0.054  \n",
      "    3. 'they' - 0.033  \n",
      "  Logit diff (correct - incorrect): 4.73 ✓\n",
      "\n",
      "Example 8:\n",
      "  Prompt: \"when rita painted a nice picture,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.265 ✓\n",
      "    2. 'and' - 0.037  \n",
      "    3. 'her' - 0.037  \n",
      "  Logit diff (correct - incorrect): 3.43 ✓\n",
      "\n",
      "Example 9:\n",
      "  Prompt: \"when rita flew a kite in the wind,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.457 ✓\n",
      "    2. 'the' - 0.049  \n",
      "    3. 'it' - 0.041  \n",
      "  Logit diff (correct - incorrect): 3.65 ✓\n",
      "\n",
      "Example 10:\n",
      "  Prompt: \"when jose wrote a letter,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. '\"' - 0.635  \n",
      "    2. 'he' - 0.066 ✓\n",
      "    3. 'saying' - 0.028  \n",
      "  Logit diff (correct - incorrect): 2.85 ✓\n",
      "\n",
      "Example 11:\n",
      "  Prompt: \"when jose followed the map,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.498 ✓\n",
      "    2. 'they' - 0.127  \n",
      "    3. 'the' - 0.060  \n",
      "  Logit diff (correct - incorrect): 3.42 ✓\n",
      "\n",
      "Example 12:\n",
      "  Prompt: \"when peter picked apples from the tree,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.251 ✓\n",
      "    2. 'they' - 0.063  \n",
      "    3. 'and' - 0.046  \n",
      "  Logit diff (correct - incorrect): 3.20 ✓\n",
      "\n",
      "Example 13:\n",
      "  Prompt: \"when leo wrote a letter,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. '\"' - 0.712  \n",
      "    2. 'he' - 0.044 ✓\n",
      "    3. 'saying' - 0.029  \n",
      "  Logit diff (correct - incorrect): 2.91 ✓\n",
      "\n",
      "Example 14:\n",
      "  Prompt: \"when leo cheered for the team,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.369 ✓\n",
      "    2. 'they' - 0.086  \n",
      "    3. 'knowing' - 0.041  \n",
      "  Logit diff (correct - incorrect): 3.52 ✓\n",
      "\n",
      "Example 15:\n",
      "  Prompt: \"when lily fed the hungry cat,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.185 ✓\n",
      "    2. 'the' - 0.046  \n",
      "    3. 'it' - 0.030  \n",
      "  Logit diff (correct - incorrect): 2.33 ✓\n",
      "\n",
      "Example 16:\n",
      "  Prompt: \"when rita helped the lost puppy,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.319 ✓\n",
      "    2. 'leaving' - 0.065  \n",
      "    3. 'the' - 0.051  \n",
      "  Logit diff (correct - incorrect): 2.64 ✓\n",
      "\n",
      "Example 17:\n",
      "  Prompt: \"when leo watered the plants,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.201 ✓\n",
      "    2. 'they' - 0.101  \n",
      "    3. 'the' - 0.064  \n",
      "  Logit diff (correct - incorrect): 2.93 ✓\n",
      "\n",
      "Example 18:\n",
      "  Prompt: \"when alex collected pretty shells,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.189 ✓\n",
      "    2. 'they' - 0.063  \n",
      "    3. 'and' - 0.049  \n",
      "  Logit diff (correct - incorrect): 3.30 ✓\n",
      "\n",
      "Example 19:\n",
      "  Prompt: \"when maria explored the forest,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.624 ✓\n",
      "    2. 'they' - 0.098  \n",
      "    3. 'the' - 0.039  \n",
      "  Logit diff (correct - incorrect): 3.96 ✓\n",
      "\n",
      "Example 20:\n",
      "  Prompt: \"when leo fed the hungry cat,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.184 ✓\n",
      "    2. 'the' - 0.035  \n",
      "    3. 'they' - 0.028  \n",
      "  Logit diff (correct - incorrect): 2.09 ✓\n",
      "\n",
      "Example 21:\n",
      "  Prompt: \"when jose fed the hungry cat,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.208 ✓\n",
      "    2. 'the' - 0.033  \n",
      "    3. 'kim' - 0.032  \n",
      "  Logit diff (correct - incorrect): 2.55 ✓\n",
      "\n",
      "Example 22:\n",
      "  Prompt: \"when maria painted a nice picture,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.234 ✓\n",
      "    2. 'it' - 0.049  \n",
      "    3. 'her' - 0.033  \n",
      "  Logit diff (correct - incorrect): 3.24 ✓\n",
      "\n",
      "Example 23:\n",
      "  Prompt: \"when lily danced at the party,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.348 ✓\n",
      "    2. 'her' - 0.090  \n",
      "    3. 'the' - 0.060  \n",
      "  Logit diff (correct - incorrect): 3.62 ✓\n",
      "\n",
      "Example 24:\n",
      "  Prompt: \"when kim solved a hard problem,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.262 ✓\n",
      "    2. 'kim' - 0.044  \n",
      "    3. 'the' - 0.037  \n",
      "  Logit diff (correct - incorrect): 3.21 ✓\n",
      "\n",
      "Example 25:\n",
      "  Prompt: \"when jose fed the hungry cat,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.208 ✓\n",
      "    2. 'the' - 0.033  \n",
      "    3. 'kim' - 0.032  \n",
      "  Logit diff (correct - incorrect): 2.55 ✓\n",
      "\n",
      "Example 26:\n",
      "  Prompt: \"when kim spotted a deer,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.052 ✓\n",
      "    2. 'looking' - 0.037  \n",
      "    3. '\"' - 0.033  \n",
      "  Logit diff (correct - incorrect): 1.35 ✓\n",
      "\n",
      "Example 27:\n",
      "  Prompt: \"when mia sailed a boat,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.457 ✓\n",
      "    2. 'the' - 0.070  \n",
      "    3. 'her' - 0.035  \n",
      "  Logit diff (correct - incorrect): 3.39 ✓\n",
      "\n",
      "Example 28:\n",
      "  Prompt: \"when jose flew a kite in the wind,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.386 ✓\n",
      "    2. 'it' - 0.059  \n",
      "    3. 'the' - 0.049  \n",
      "  Logit diff (correct - incorrect): 3.48 ✓\n",
      "\n",
      "Example 29:\n",
      "  Prompt: \"when maria collected pretty shells,\"\n",
      "  Correct: 'she' | Incorrect: 'he'\n",
      "  Top 3 predictions:\n",
      "    1. 'she' - 0.203 ✓\n",
      "    2. 'they' - 0.069  \n",
      "    3. 'and' - 0.040  \n",
      "  Logit diff (correct - incorrect): 2.76 ✓\n",
      "\n",
      "Example 30:\n",
      "  Prompt: \"when leo presented to the class,\"\n",
      "  Correct: 'he' | Incorrect: 'she'\n",
      "  Top 3 predictions:\n",
      "    1. 'he' - 0.425 ✓\n",
      "    2. 'they' - 0.070  \n",
      "    3. 'she' - 0.052 ✗\n",
      "  Logit diff (correct - incorrect): 2.10 ✓\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# DUMMY PRONOUN TASK EXAMPLES\n",
    "# ========================================\n",
    "from sparse_pretrain.src.pruning.tasks import get_task\n",
    "\n",
    "# Load the task\n",
    "task = get_task(\"dummy_pronoun\", tokenizer, seed=42, split=\"val\")\n",
    "print(f\"Task: {task.name}\")\n",
    "print(f\"Templates: {len(task.templates)}\")\n",
    "\n",
    "# Generate examples and show predictions\n",
    "n_examples = 30\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Showing {n_examples} examples from dummy pronoun task\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i in range(n_examples):\n",
    "    ex = task.generate_example()\n",
    "    \n",
    "    # Get the prompt (everything before the final token)\n",
    "    prompt_ids = ex.positive_ids[:]  # All but last token\n",
    "    prompt_text = tokenizer.decode(prompt_ids)\n",
    "    \n",
    "    # Get correct and incorrect tokens\n",
    "    correct_token = tokenizer.decode([ex.correct_token])\n",
    "    incorrect_token = tokenizer.decode([ex.incorrect_token])\n",
    "    \n",
    "    # Run forward pass\n",
    "    input_ids = prompt_ids.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "        logits = output[0] if isinstance(output, tuple) else output\n",
    "        final_logits = logits[0, -1, :]\n",
    "    \n",
    "    probs = torch.softmax(final_logits, dim=-1)\n",
    "    top_probs, top_indices = torch.topk(probs, 3)\n",
    "    \n",
    "    # Get logits for correct/incorrect\n",
    "    correct_logit = final_logits[ex.correct_token].item()\n",
    "    incorrect_logit = final_logits[ex.incorrect_token].item()\n",
    "    logit_diff = correct_logit - incorrect_logit\n",
    "    \n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Prompt: \\\"{prompt_text}\\\"\")\n",
    "    print(f\"  Correct: '{correct_token}' | Incorrect: '{incorrect_token}'\")\n",
    "    print(f\"  Top 3 predictions:\")\n",
    "    for j, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = tokenizer.decode([idx.item()])\n",
    "        marker = \"✓\" if idx.item() == ex.correct_token else (\"✗\" if idx.item() == ex.incorrect_token else \" \")\n",
    "        print(f\"    {j+1}. '{token}' - {prob.item():.3f} {marker}\")\n",
    "    print(f\"  Logit diff (correct - incorrect): {logit_diff:.2f} {'✓' if logit_diff > 0 else '✗'}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

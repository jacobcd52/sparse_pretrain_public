{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Task Sentences\n",
        "\n",
        "Interactive notebook to test sentences and see model predictions with log probabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading model: jacobcd52/ss_dense\n",
            "Loading standalone model from HuggingFace Hub: jacobcd52/ss_dense\n",
            "Model loaded!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add project root to path\n",
        "sys.path.insert(0, str(Path.cwd().parent.parent))\n",
        "\n",
        "from sparse_pretrain.src.pruning.run_pruning import load_model\n",
        "\n",
        "# Load model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_path = \"jacobcd52/ss_d128_f1\"\n",
        "tokenizer_name = \"SimpleStories/SimpleStories-1.25M\"\n",
        "\n",
        "print(f\"Loading model: {model_path}\")\n",
        "model, config = load_model(model_path, device=device)\n",
        "model.eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"Model loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model_config': {'n_layer': 4,\n",
              "  'd_model': 512,\n",
              "  'n_ctx': 512,\n",
              "  'd_head': 16,\n",
              "  'd_mlp': 2048,\n",
              "  'vocab_size': 4096,\n",
              "  'use_rms_norm': True,\n",
              "  'tie_embeddings': False,\n",
              "  'use_positional_embeddings': False,\n",
              "  'use_bigram_table': False,\n",
              "  'use_attention_sinks': True,\n",
              "  'activation': 'gelu',\n",
              "  'dropout': 0.0,\n",
              "  'use_bias': True},\n",
              " 'sparsity_config': {'enable_weight_sparsity': False,\n",
              "  'target_l0_fraction': 1,\n",
              "  'enable_activation_sparsity': False,\n",
              "  'activation_topk_fraction': 0.25},\n",
              " 'training_config': {'total_tokens': 2000000000,\n",
              "  'batch_size': 128,\n",
              "  'dataset_name': 'SimpleStories/SimpleStories',\n",
              "  'tokenizer_name': 'SimpleStories/SimpleStories-1.25M'}}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_next_token_probs(context: str, top_k: int = 20):\n",
        "    \"\"\"\n",
        "    Get top-k next token predictions with probabilities.\n",
        "    \n",
        "    Returns:\n",
        "        List of (token_str, probability, log_prob) tuples\n",
        "    \"\"\"\n",
        "    input_ids = tokenizer.encode(context, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    input_ids = input_ids.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        if isinstance(outputs, tuple):\n",
        "            logits = outputs[0]\n",
        "        else:\n",
        "            logits = outputs\n",
        "        logits = logits[:, -1, :]  # (1, vocab_size)\n",
        "        probs = F.softmax(logits, dim=-1)[0]\n",
        "        log_probs = F.log_softmax(logits, dim=-1)[0]\n",
        "    \n",
        "    # Get top-k\n",
        "    top_probs, top_indices = torch.topk(probs, top_k)\n",
        "    \n",
        "    results = []\n",
        "    for i in range(top_k):\n",
        "        token_id = top_indices[i].item()\n",
        "        token_str = tokenizer.decode([token_id])\n",
        "        prob = top_probs[i].item()\n",
        "        log_prob = log_probs[token_id].item()\n",
        "        results.append((token_str, prob, log_prob))\n",
        "    \n",
        "    return results, probs, log_probs\n",
        "\n",
        "\n",
        "def show_predictions(context: str, top_k: int = 15):\n",
        "    \"\"\"Show top-k predictions for a context.\"\"\"\n",
        "    print(f\"Context: \\\"{context}\\\"\")\n",
        "    print(f\"\\nTop {top_k} predictions:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    results, probs, log_probs = get_next_token_probs(context, top_k)\n",
        "    \n",
        "    for i, (token_str, prob, log_prob) in enumerate(results):\n",
        "        display_token = repr(token_str)\n",
        "        print(f\"{i+1:2}. {display_token:15} P={prob:.4f}  log P={log_prob:.2f}\")\n",
        "    \n",
        "    return probs, log_probs\n",
        "\n",
        "\n",
        "def compare_tokens(context: str, tokens: list):\n",
        "    \"\"\"Compare probabilities for multiple tokens.\"\"\"\n",
        "    print(f\"Context: \\\"{context}\\\"\")\n",
        "    print(f\"\\nComparing tokens:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    _, probs, log_probs = get_next_token_probs(context, top_k=1)\n",
        "    \n",
        "    for token in tokens:\n",
        "        token_ids = tokenizer.encode(token, add_special_tokens=False)\n",
        "        if len(token_ids) == 0:\n",
        "            print(f\"  {repr(token):15} NOT FOUND\")\n",
        "            continue\n",
        "        \n",
        "        token_id = token_ids[0]\n",
        "        prob = probs[token_id].item()\n",
        "        log_prob = log_probs[token_id].item()\n",
        "        print(f\"  {repr(token):15} P={prob:.4f}  log P={log_prob:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: \"when leo was hiding in the bush, mia approached\"\n",
            "\n",
            "Top 15 predictions:\n",
            "--------------------------------------------------\n",
            " 1. 'him'           P=0.6026  log P=-0.51\n",
            " 2. '.'             P=0.1171  log P=-2.14\n",
            " 3. 'the'           P=0.0601  log P=-2.81\n",
            " 4. 'leo'           P=0.0409  log P=-3.20\n",
            " 5. 'slowly'        P=0.0317  log P=-3.45\n",
            " 6. 'it'            P=0.0244  log P=-3.71\n",
            " 7. 'with'          P=0.0149  log P=-4.21\n",
            " 8. 'quietly'       P=0.0146  log P=-4.23\n",
            " 9. 'her'           P=0.0130  log P=-4.34\n",
            "10. ','             P=0.0129  log P=-4.35\n",
            "11. 'them'          P=0.0127  log P=-4.37\n",
            "12. 'and'           P=0.0113  log P=-4.48\n",
            "13. 'carefully'     P=0.0102  log P=-4.58\n",
            "14. 'cautious'      P=0.0060  log P=-5.12\n",
            "15. 'his'           P=0.0036  log P=-5.63\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([1.1741e-08, 2.1459e-06, 8.4641e-05,  ..., 3.5491e-09, 2.0298e-07,\n",
              "         2.9445e-08], device='cuda:0'),\n",
              " tensor([-18.2602, -13.0520,  -9.3771,  ..., -19.4566, -15.4102, -17.3407],\n",
              "        device='cuda:0'))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test a sentence - edit this!\n",
        "context = \"when leo was hiding in the bush, mia approached\"\n",
        "\n",
        "show_predictions(context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context: \"when Leo went to the park,\"\n",
            "\n",
            "Comparing tokens:\n",
            "--------------------------------------------------\n",
            "  ' he'           P=0.6685  log P=-0.40\n",
            "  ' she'          P=0.0195  log P=-3.94\n",
            "  ' they'         P=0.0488  log P=-3.02\n",
            "  ' Leo'          P=0.0302  log P=-3.50\n"
          ]
        }
      ],
      "source": [
        "felt, rubbed, shook, raised\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

================================================================================
REPLICATION GUIDE (GRAPH PRUNING): "Weight-sparse transformers have interpretable circuits"
Gao et al. (2025) - OpenAI
================================================================================

This document summarizes the paper’s graph/circuit pruning methodology: the
structured, node-mask-based procedure used to isolate minimal task circuits,
including all pruning-specific hyperparameters and appendix details (main text
Section 2.2 “Pruning”, Appendix A.5, Appendix E, Appendix F.2).

Paper source in this workspace: for_claude/gao.pdf (text extract: for_claude/gao.txt)

IMPORTANT SCOPE NOTE
--------------------
- This guide covers ONLY the pruning / circuit-isolation (“graph pruning”)
  procedure, not weight-sparse pretraining or bridges.
- Wherever the paper does not specify a detail (e.g., exact mask-training steps),
  this guide explicitly marks it as NOT SPECIFIED IN PAPER TEXT.

================================================================================
SECTION 0: GOAL + OUTPUTS
================================================================================

Goal (main text, Section 2.2; Appendix A.5):
- For each hand-crafted binary next-token task, find the smallest circuit that
  achieves a target loss on the task distribution.
- The circuit is defined as a set of “nodes” connected by “edges” (nonzero
  weights) in the model’s computation graph.

Outputs you should expect to produce per (model, task, target-loss):
- A binary inclusion mask for each “node” location (Appendix A.5).
- A pruned circuit whose “deleted” nodes are replaced by mean ablations
  (main text Section 2.2).
- Circuit size metrics (primarily number of edges), plus task loss of the pruned
  circuit at the chosen target.
- A final scale+shift calibration on logits after discretization (Appendix A.5).

================================================================================
SECTION 1: DEFINITIONS (CIRCUIT / GRAPH PRUNING)
================================================================================

1.1 TASK SETUP (context)
------------------------
The pruning procedure is run on a curated suite of 20 simple “binary completion”
tasks (main text Section 2.2; Table 1 in the paper). Each task datapoint is a
paired (positive, negative) sequence that forces the model to choose between two
completions (replication context: see for_claude/replication_guide.txt Section 7).

Pruning target loss (main text Section 2.2):
- Target task loss is 0.15 everywhere unless otherwise specified.

1.2 CIRCUITS AS NODES + EDGES
-----------------------------
Circuit definition (main text, around Figure 4):
- A circuit is a set of nodes connected by edges.

Node definition (main text, around Figure 4; plus Appendix A.5):
- Nodes are “maximally granular” and correspond to the following kinds of
  computation elements:
  - An individual MLP neuron (post-GELU activation).
  - An individual attention channel (at the level of Q/K/V activations).
  - A residual channel read (corresponding to a row of an MLP input matrix, e.g.,
    a row of c_fc).
  - A residual channel write (corresponding to a column of an MLP output matrix,
    e.g., a column of c_proj).

Edge definition (main text, around Figure 4):
- An edge is a nonzero entry in a weight matrix that connects two nodes.

Note on how pruning is applied (Appendix A.5):
- The learned masks are defined at the node level (not at edge level).
- The same mask is applied uniformly across all prompts and across all tokens.

1.3 INTERPRETABILITY METRIC USED IN THE PAPER
---------------------------------------------
Main quantitative interpretability metric (main text, around Figure 4):
- Geometric mean of the number of edges in the circuit across the curated tasks.
- Lower is “more interpretable”.

================================================================================
SECTION 2: WHERE MASKS ARE INSERTED (NODE LOCATIONS)
================================================================================

Appendix A.5 (“Setup”) specifies the mask insertion points:

Boolean masks are inserted at:
- Immediately after each RMSNorm in the attention and MLP blocks.
- At the very end of each attention and MLP block, right before adding the result
  back to the residual stream.
- After each MLP activation.
- After the attention q, k, v activations.

Each element of the mask at each of these locations is treated as a distinct
“node” (Appendix A.5).

Footnote / nuance (Appendix A.5, footnote 17):
- Since q and k channels are paired most of the time, each attention qk pair
  generally uses two nodes.
- In retrospect, queries and keys should have used tied masks.

================================================================================
SECTION 3: WHAT “DELETING A NODE” MEANS (MEAN ABLATION)
================================================================================

3.1 ABLATION METHOD
-------------------
Main text Section 2.2 specifies:
- Deleted nodes are mean-ablated.
- That is, a deleted node’s activation is frozen to its mean activation over the
  pretraining distribution.

Appendix A.5 additionally specifies:
- The mask is applied uniformly across all prompts and across all tokens.

NOT SPECIFIED IN PAPER TEXT (important):
- Exactly how the per-node mean activation is estimated in practice (e.g., how
  many batches/tokens, which sampling scheme, whether done per-position or pooled).

3.2 WHY MEAN ABLATION (FAITHFULNESS DISCUSSION)
----------------------------------------------
Appendix E discusses mean ablations as a faithfulness proxy:
- It is inspired by Causal Scrubbing (Chan et al., 2022) but is weaker than full
  distributional substitution.
- The paper argues mean ablation is practically easier to satisfy than full
  substitution, and that the loss in faithfulness is “not so bad”, while noting
  it is not perfect.
- Mean ablation tends to yield more complete circuits than activation patching,
  because patching can miss circuit components whose activations differ across
  prompt pairs (Appendix E).

3.3 NECESSITY / SUFFICIENCY VALIDATION (INVERSE PRUNING)
--------------------------------------------------------
The paper validates circuits via:
- Sufficiency: mean-ablating everything except the circuit preserves task loss
  (main text; discussed with Figure 32 context).
- Necessity (“Inverse Pruning”): ablating ONLY the circuit nodes harms loss
  severely (Figure 32).

================================================================================
SECTION 4: MASK PARAMETERIZATION + TRAINING OBJECTIVE
================================================================================

4.1 MASK PARAMETERS AND FORWARD GATING
--------------------------------------
Main text Section 2.2 describes the structured pruning setup:
- Learn a set of mask parameters τ_i, indexed by node.
- Gate the corresponding node activation x_i via:
  x_i  ↦  x_i ⊙ σ(τ_i)
  where σ is the Heaviside step function (hard threshold).

Appendix A.5 adds:
- Each node has a learned parameter clamped to [-1, 1].
- After clamping, the boolean mask is computed by passing the parameter through
  a Heaviside step function.
- Clamping to [-1, 1] is enforced after every training step.

NOT SPECIFIED IN PAPER TEXT:
- Exact convention for the Heaviside threshold (e.g., whether H(0)=0 or 1, or
  any tie-breaking).

4.2 BACKPROP THROUGH THE STEP FUNCTION (SURROGATE GRADIENT)
-----------------------------------------------------------
Main text Section 2.2:
- Backprop through the Heaviside using a sigmoid-derivative surrogate gradient,
  analogous to a Straight-Through Estimator (Bengio et al., 2013).

Appendix A.5 (“Optimization”) specifies:
- Use a sigmoid estimator to compute a biased gradient approximation in the
  backward pass, with temperature `heaviside_temp`.

NOT SPECIFIED IN PAPER TEXT:
- Exact sigmoid estimator formula and how temperature is applied.

4.3 PRUNING LOSS FUNCTION
-------------------------
Appendix A.5 (“Loss function”) specifies the objective:
- Optimize a linear combination of:
  - Task cross-entropy loss
  - k = number of nonzero elements in the mask
- Weighting between these is controlled by `k_coef`.

NOT SPECIFIED IN PAPER TEXT:
- Exact definition of “task cross-entropy” for the paired binary tasks (e.g.,
  whether CE is applied on the final token only, logit-difference loss, etc.).

================================================================================
SECTION 5: MASK OPTIMIZATION PROCEDURE (INNER LOOP)
================================================================================

5.1 INITIALIZATION
------------------
Appendix A.5 (“Initialization”) specifies:
- Initialize mask parameters with Gaussian noise:
  - Standard deviation scaled by `init_noise_scale`
  - Mean shifted/centered at `init_noise_bias`
- Then clamp to [-1, 1].

5.2 OPTIMIZER + LR SCHEDULE
---------------------------
Appendix A.5 (“Optimization”) specifies:
- Optimizer: AdamW
- Use grad clipping
- Learning rate `lr` linearly decays through training
- No warmup is used (as written in Appendix A.5)

NOT SPECIFIED IN PAPER TEXT:
- AdamW β1, β2, ε values for pruning (only `inv_beta2` appears as a tuned HP).
- Whether weight decay is applied to the mask parameters and/or other params.
- Exact grad clipping rule and threshold value.
- Total number of pruning optimization steps per run.

5.3 PRUNING BATCHING DETAILS
----------------------------
Appendix A.5 (hyperparameter optimization paragraph) specifies:
- Batch size: 64 task datapoints
- Each datapoint consists of (positive sequence, negative sequence)
- Total sequences per batch: 128
- Sequence length: up to 256 tokens

================================================================================
SECTION 6: DISCRETIZATION + CALIBRATION (POST-PROCESSING)
================================================================================

6.1 MASK DISCRETIZATION VIA BISECTION
-------------------------------------
Appendix A.5 (“Mask discretization”) specifies:
- After training the (continuous) mask parameters, discretize by bisection over k
  to find the k that exactly achieves the threshold (target loss).
- The resulting k can diverge substantially from the k implied by the final
  continuous training state.

NOT SPECIFIED IN PAPER TEXT:
- The exact bisection procedure (e.g., whether you threshold τ_i, or pick top-k
  by τ_i, and how ties are handled).
- Whether threshold is “≤ target loss” or “closest to target”, and any tolerance.

6.2 LOGIT SCALE+SHIFT CALIBRATION
---------------------------------
Appendix A.5 specifies:
- Discretized pruned models are often “quite uncalibrated”.
- Fit a scale+shift transformation on final logits using 16 steps of LBFGS
  (Liu & Nocedal, 1989).
- The paper notes it is unclear if this is principled in general.

Interpretation (consistent with Appendix F.2 discussion):
- This logit recalibration is a small post-hoc adjustment that can compensate for
  distribution shift induced by discretization, but it changes output calibration.

NOT SPECIFIED IN PAPER TEXT:
- Whether the LBFGS objective is the task loss on the task distribution, and how
  the calibration data is selected (train vs held-out).
- Whether scale+shift is shared across tokens/positions or only the final token.

================================================================================
SECTION 7: HYPERPARAMETER OPTIMIZATION WITH CARBS (OUTER LOOP)
================================================================================

Appendix A.5 (“Hyperparameter optimization”) specifies:
- Use CARBS (Fetterman et al., 2023) to retune hyperparameters for each (model, task).
- Run 32 iterations of CARBS with 8 parallel pruning jobs per iteration.
  Footnote 18 clarifies:
  - 256 total steps of the CARBS optimizer.
  - Alternate: generate 8 suggestions → run all 8 → update → repeat.
- Results are generally poor in early iterations and improve dramatically toward
  the end of CARBS tuning.

Initial CARBS search centers (Appendix A.5, Table 2):
- k_coef: 1 × 10^-4
- init_noise_scale: 1 × 10^-2
- init_noise_bias: 1 × 10^-1
- wd: 1 × 10^-3
- lr: 3 × 10^-3
- inv_beta2: 5 × 10^-2
- lr_warmup_frac: 5 × 10^-2
- heaviside_temp: 1 × 10^0

Important nuance / possible tension (paper text):
- Appendix A.5 states “without any warmup” for mask optimization, but Table 2
  includes `lr_warmup_frac` as a tuned hyperparameter.
- The paper text does not reconcile this discrepancy; treat it as
  NOT SPECIFIED IN PAPER TEXT how warmup is handled in the actual runs.

NOT SPECIFIED IN PAPER TEXT:
- The full CARBS search ranges / priors / parameterization, beyond initial centers.
- The exact scalar objective CARBS optimizes (e.g., edge count at target loss, or
  a combined objective across loss + size), and whether it averages over seeds.

================================================================================
SECTION 8: OPTIONAL / PRACTICAL DETAILS FROM APPENDICES (QUALITATIVE)
================================================================================

8.1 MEAN ABLATION VS ACTIVATION PATCHING (Appendix E)
-----------------------------------------------------
Appendix E claims:
- Mean ablation yields more complete circuits than activation patching because
  patching can miss cross-pair differences (example: variable-name copying in
  set_or_string tasks).

8.2 MANUAL CLEANUP / RESCALING AND REDUNDANCY (Appendix F.2)
------------------------------------------------------------
Appendix F.2 describes additional interventions used when presenting some
qualitative circuit diagrams:
- Rescaling a small number of nodes can remove redundant components.
- This can be powerful and risks “hiding” superposition if applied broadly.
- For bracket_counting, they rescale specific residual-delta channels to simplify
  the circuit and remove redundancy (details are task-specific in Appendix F.2).

This is conceptually related to the pruning pipeline’s logit scale+shift
recalibration: both are post-hoc linear adjustments that can recover loss.

NOT SPECIFIED IN PAPER TEXT:
- A general algorithmic prescription for when to do these manual simplifications,
  beyond the qualitative descriptions and specific examples.

================================================================================
SECTION 9: MINIMAL “DO THIS TO REPLICATE” CHECKLIST
================================================================================

Given a trained model and a binary task distribution, to replicate Appendix A.5:

1) Insert node masks at the four locations listed in Section 2.
2) Define per-node mean activations from the pretraining distribution and use
   mean ablation for masked-out nodes (paper specifies mean ablation but not the
   estimation details).
3) Parameterize each node mask with τ_i clamped to [-1, 1], and gate activations
   using a Heaviside step (forward) with a sigmoid-estimator surrogate gradient
   (backward) using `heaviside_temp`.
4) Optimize τ with AdamW + grad clipping, linear LR decay, and objective:
     task_cross_entropy + k_coef * (# nonzero mask entries)
5) Discretize after training by bisection over k to hit the target loss exactly.
6) Recalibrate logits with a global scale+shift fit via 16 LBFGS steps.
7) Tune the hyperparameters per (model, task) using CARBS:
   32 iterations, 8 parallel pruning jobs/iteration, seeded at Table 2 centers.

================================================================================
END OF GRAPH PRUNING REPLICATION GUIDE
================================================================================



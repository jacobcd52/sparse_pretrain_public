================================================================================
REPLICATION GUIDE: "Weight-sparse transformers have interpretable circuits"
Gao et al. (2025) - OpenAI
================================================================================

This document contains all information required to exactly replicate the 
weight-sparse transformer training and circuit pruning methodology from the 
paper. This guide excludes the "bridges" methodology for coupling to dense 
models.

Code repository: https://github.com/openai/circuit_sparsity/

================================================================================
SECTION 1: ARCHITECTURE
================================================================================

1.1 BASE ARCHITECTURE
---------------------
- Architecture type: GPT-2 style decoder-only transformer
- Similar to Radford et al. (2019) with modifications listed below

1.2 DEFAULT MODEL DIMENSIONS (for most experiments)
---------------------------------------------------
- n_layer = 8 (number of transformer layers)
- d_model = 2048 (hidden/residual stream dimension)
- n_ctx = 256 (context length / sequence length)
- d_head = 16 (attention head dimension - smaller than typical for monosemanticity)
- n_heads = d_model / d_head = 128 (number of attention heads)

1.3 NORMALIZATION
-----------------
- Use RMSNorm instead of LayerNorm
- Reason: Ensures zero values have privileged meaning in residual stream
- RMSNorm reference: Zhang & Sennrich (2019)
- Important: RMSNorm weights can be folded into MLP/attention weights without 
  altering the weight L0

1.4 EMBEDDINGS
--------------
- Embedding and unembedding matrices are UNTIED (separate parameters)
- Token embeddings are also subject to weight sparsity constraints
- Positional embeddings: NOT USED in most experiments
  - Reference: Haviv et al. (2022) shows models work without positional encodings
  - This is roughly neutral on loss (see Figure 26 in paper)
  - Alternative (used in some earlier experiments): Learned absolute positional 
    encodings concatenated to residual stream as read-only channels

1.5 BIGRAM TABLE
----------------
- Add a dense d_vocab × d_vocab matrix (bigram table)
- The most recent token's entries are added directly to final logits
- Purpose: Avoids sparse parameters needing to memorize bigram frequencies
- This is a DENSE component with simple interpretation (prior log probs over bigrams)
- Improves loss without harming interpretability (see Figure 24)

1.6 ATTENTION SINKS (Optional but recommended)
----------------------------------------------
- Per-head learnable attention denominator bias
- Reference: Xiao et al. (2023)
- Effect: Leads to cleaner attention circuits without impacting loss substantially
- See Figure 18 for ablation

1.7 ACTIVATION FUNCTION LOCATIONS (AbsTopK)
-------------------------------------------
The AbsTopK activation function is applied at these locations:
- After each RMSNorm in attention and MLP blocks
- At the end of each attention and MLP block (before adding to residual stream)
- After MLP activation (post-GELU)
- After attention Q, K, V projections (applied separately to each)

See Figure 12 in paper for visual illustration.

================================================================================
SECTION 2: WEIGHT SPARSITY
================================================================================

2.1 L0 CONSTRAINT ENFORCEMENT
-----------------------------
- Method: After each optimizer step, zero out all but the largest magnitude 
  entries in each weight matrix
- Every matrix has the SAME fraction of nonzero elements
- Sparsity is applied to ALL weights and biases, including token embeddings
- Sparsest models: approximately 1 in 1000 nonzero weights

2.2 SPARSITY ANNEALING SCHEDULE
-------------------------------
- Start with fully dense weights
- Linearly anneal L0 from dense → target L0 over first 50% of training
- For largest/sparsest models: increase to 80% of training
- See Figure 17 and Figure 23 for ablations

2.3 MINIMUM NONZERO WEIGHTS PER NEURON
--------------------------------------
- Parameter j = 4 (default)
- Never zero out values that would cause a neuron or attention channel to have 
  fewer than j nonzero values
- Purpose: Reduces chance of dead neurons
- Note: This may artificially keep some irrelevant neurons alive
- See Figure 22 for ablation of j values (1, 2, 4, none)

2.4 BUG NOTE
------------
There was a bug in L0 scheduling where embedding, unembedding, and biases would 
go from dense to sparse in a relatively small number of steps in the middle of 
training (rather than following the intended schedule). Fixing this bug slightly 
hurt model quality, so it was kept. See Figure 21.

================================================================================
SECTION 3: ACTIVATION SPARSITY
================================================================================

3.1 AbsTopK ACTIVATION FUNCTION
-------------------------------
- At each designated location, zero out all but the k largest values by magnitude
- Default k = 1/4 of the dimension at each location (i.e., 1 in 4 activations nonzero)

3.2 PLACEMENT
-------------
AbsTopK is inserted at each "node" position, between each operation:
1. After RMSNorm (before attention/MLP computation)
2. After Q, K, V projections (separately for each)
3. After attention operation
4. After MLP fc layer
5. After MLP activation (GELU)
6. After MLP c_proj layer

3.3 INTERACTION WITH WEIGHT SPARSITY
------------------------------------
- Some activation sparsity helps on top of weight sparsity
- Too much activation sparsity hurts the capability-interpretability frontier
- Weight sparsity naturally induces activation sparsity (higher kurtosis)
- See Figure 37 for Pareto frontier analysis

================================================================================
SECTION 4: OPTIMIZATION
================================================================================

4.1 OPTIMIZER: AdamW
--------------------
- Reference: Loshchilov & Hutter (2019); Kingma & Ba (2014)
- β1 = 0.9
- β2 = 0.95
- Weight decay λ = 0.1
- ε = 0.1 (NOTE: unusually large epsilon!)
- Learning rate: Swept for every experiment (no single default)

See ablations in Figures 27, 28, 29, 30.

4.2 GRADIENT CLIPPING
---------------------
- Clip root-mean-square of gradient to 1
- This is ESSENTIAL for training stability
- See Figure 16 for ablation

4.3 LEARNING RATE SCHEDULE ("Sharkfin")
---------------------------------------
The LR schedule is the product of two factors:
1. Normal warmup-decay schedule
2. Factor of 1/√L0

The 1/√L0 factor is necessary - smaller L0 requires larger learning rates.
This change is necessary to get benefit from L0 annealing (see Figure 25).

4.4 WARMUP
----------
- Warmup for first 1% of training (default)
- Critical for stability at higher learning rates
- Longer warmup (5%, 10%) is slightly beneficial
- See Figures 13 and 14

4.5 DECAY
---------
- Standard linear decay after warmup
- See Figure 15 for no-decay ablation

4.6 GRADIENT AND MOMENT HANDLING
--------------------------------
- Gradients and Adam moments are kept DENSE (not modified from standard implementation)
- Only the weights themselves are sparsified

================================================================================
SECTION 5: DATASET
================================================================================

5.1 DATA SOURCE
---------------
- Python code generated from GPT-4-base and GPT-4o-base

5.2 DATASET COMPOSITION
-----------------------
Two components:
1. 10 billion tokens of Python code with simple/repetitive idioms
   - Inspired by TinyStories (Eldan & Li, 2023)
   - Makes it easier to observe interesting circuits at smaller scale
2. 25 billion tokens closer to full distribution of Python code

Total: 35 billion tokens

5.3 TOKENIZER
-------------
- BPE tokenizer
- Vocabulary size: 2048 tokens
- Trained on the dataset

================================================================================
SECTION 6: PRUNING ALGORITHM
================================================================================

6.1 GOAL
--------
For each task, find the smallest circuit (subset of nodes) that achieves a 
target loss on the task distribution.

6.2 NODE DEFINITION
-------------------
Nodes are maximally granular, corresponding to rows/columns of weight matrices:
- Individual neurons (post-GELU activations in MLP)
- Attention channels (Q, K, V channels)
- Residual channel reads (rows of c_fc)
- Residual channel writes (columns of c_proj)

An edge is a nonzero entry in a weight matrix connecting two nodes.

6.3 MASK LOCATIONS
------------------
Boolean masks are inserted at:
- Immediately after each RMSNorm in attention and MLP blocks
- At the end of each attention and MLP block (before adding to residual)
- After each MLP activation
- After attention Q, K, V activations

Note: Q and K channels are paired most of the time, so each attention QK pair 
uses two nodes. In retrospect, queries and keys should have used tied masks.

6.4 ABLATION METHOD
-------------------
- Deleted nodes are MEAN-ABLATED
- Activation is frozen at the mean activation over the pretraining distribution
- Mask is applied uniformly across all prompts and all tokens

6.5 MASK OPTIMIZATION
---------------------
- Learn a parameter for each node, clamped to [-1, 1]
- Parameter is passed through Heaviside step function to get boolean mask
- Backprop through Heaviside using sigmoid-derivative surrogate gradient
  (similar to Straight-Through Estimator from Bengio et al., 2013)
- Temperature parameter: heaviside_temp

6.6 INITIALIZATION
------------------
- Initialize with Gaussian noise scaled by init_noise_scale
- Centered at init_noise_bias
- Clamp mask to [-1, 1] after every training step

6.7 LOSS FUNCTION
-----------------
Linear combination of:
1. Task cross-entropy loss
2. k (number of nonzero elements in mask)

Weighted by k_coef parameter.

6.8 OPTIMIZATION SETTINGS
-------------------------
- Optimizer: AdamW with gradient clipping
- Learning rate: Linear decay through training, NO warmup
- Batch size: 64 task datapoints (each = positive + negative sequence)
- Total sequences: 128 sequences of length up to 256 tokens

6.9 POST-TRAINING DISCRETIZATION
--------------------------------
After training:
1. Bisect for the k that exactly achieves the threshold loss
   (this can diverge substantially from final k from training)
2. Optimize scale+shift transformation to final logits using 16 steps of LBFGS
   (because discretized models are often uncalibrated)

6.10 HYPERPARAMETER OPTIMIZATION
--------------------------------
Use CARBS (Fetterman et al., 2023) to tune hyperparameters for each model+task.
- Run 32 iterations of CARBS
- 8 parallel pruning jobs per iteration (256 total optimizer steps)
- Alternate: generate 8 suggestions → run all → update → repeat

Initial CARBS search centers:
+---------------------+---------------+
| Hyperparameter      | Search Center |
+---------------------+---------------+
| k_coef              | 1e-4          |
| init_noise_scale    | 1e-2          |
| init_noise_bias     | 1e-1          |
| wd (weight decay)   | 1e-3          |
| lr                  | 3e-3          |
| inv_beta2           | 5e-2          |
| lr_warmup_frac      | 5e-2          |
| heaviside_temp      | 1.0           |
+---------------------+---------------+

6.11 TARGET LOSS
----------------
Default target task loss: 0.15

================================================================================
SECTION 7: EVALUATION TASKS
================================================================================

7.1 TASK STRUCTURE
------------------
- 20 hand-crafted Python next-token binary prediction tasks
- Each task forces model to choose between one of two completions
- Task datapoints consist of positive and negative sequence pairs

7.2 TASK LIST
-------------
+---------------------------+--------------------------------------------------+
| Task Name                 | Description                                      |
+---------------------------+--------------------------------------------------+
| single_double_quote       | Predict ') or ") to close string               |
| final_kwarg               | Predict =None vs ):\n based on kwargs          |
| set_or_string             | Track set() vs string, predict .add vs +=      |
| set_or_string_fixedvarname| Same but variable name always the same          |
| class_init                | Copy variable name in self.x = x patterns       |
| with_as                   | Distinguish with-as statements from var decls   |
| bracket_brace             | Predict : or , based on dict vs list           |
| with_open                 | Predict 'r' or 'w' based on file name          |
| bracket_counting          | Predict ] or ]] based on nesting depth         |
| for_while                 | Predict 'in' vs ':' based on for vs while      |
| else_elif                 | Predict ':' vs not based on else vs elif       |
| fstring_brace             | Predict if print uses f-string                 |
| if_ternary                | Predict ':' based on if inside ternary         |
| var_swap                  | Predict x in x, y = y, x pattern               |
| indent_for                | Predict correct indentation after for loop      |
| lambda_func               | Distinguish lambda from function definition     |
| if_equals                 | Predict == vs = based on if vs assignment      |
| enumerate_range           | Predict enumerate vs range based on vars       |
| var_if                    | Distinguish if conditional from var declaration |
| while_return_true         | Predict indentation after while/return true    |
+---------------------------+--------------------------------------------------+

7.3 TOKENIZATION CONSIDERATIONS
-------------------------------
- single_double_quote: Use (" and (' tokens (always tokenize as single tokens)
- bracket_counting: Note that ], is one token, so ] unlikely to end non-final row

7.4 INTERPRETABILITY METRIC
---------------------------
- Primary metric: Geometric mean of number of edges in circuit across all tasks
- Lower is better (more interpretable)

================================================================================
SECTION 8: SCALING OBSERVATIONS
================================================================================

8.1 L0 vs TOTAL PARAMETERS
--------------------------
- Decreasing L0 (more sparse): trades capability for interpretability
- Increasing total parameters with fixed L0: improves BOTH capability and interpretability
- The larger model with same L0 is strictly more expressive
- Fewer nonzero weights per neuron/residual channel in larger models

8.2 BITS TO SPECIFY MODEL
-------------------------
Number of bits to specify which parameters are nonzero ≈ O(L0 × log(N))
where N is total number of parameters.

8.3 TESTED L0 VALUES
--------------------
From paper figures: 0.9M, 1.9M, 3.7M, 7.4M, 14.8M nonzero parameters

8.4 TESTED TOTAL PARAMETER COUNTS
---------------------------------
From paper figures: 7.4M, 29.7M, 118.8M, 475.1M, 1900.5M total parameters

8.5 COMPUTE EFFICIENCY NOTE
---------------------------
- Sparse models require 100-1000x more training/inference compute than dense
- Many dead neurons reduce training efficiency
- Current optimization doesn't maximally explore different sparsity masks

================================================================================
SECTION 9: FEATURE BINARIZATION (Validation Method)
================================================================================

9.1 PURPOSE
-----------
Test whether features carry information in magnitude beyond on/off state.

9.2 ALGORITHM
-------------
Construct activation function ψ_t,ℓ,r where:
- ψ_1 is identity function
- ψ_0 is step function: ψ_0(x) = ℓ if x < (ℓ+r)/2, else r

Mix identity with appropriately-shifted sigmoid of temperature t.
Anneal t as (1 - progress)^5 throughout training.

9.3 INITIALIZATION
------------------
For each node:
1. Try thresholds at 1/4, 1/2, 3/4 between max and min observed activations
2. Take mean of activations above/below threshold for r and ℓ respectively
3. Choose the threshold that results in least performance drop

9.4 FINDINGS
------------
- Generally possible to binarize many tasks
- Greater binarizability as total params increases (at fixed L0)
- Increasing L0 (at fixed total params) improves binarizability
- Some features are not binarizable but still understandable (e.g., bracket counting 
  with 3 distinct semantic values for different nesting depths)

================================================================================
SECTION 10: ALTERNATIVE METHODS TESTED
================================================================================

10.1 L0 REGULARIZATION (Louizos et al., 2018)
---------------------------------------------
Used HardConcrete distribution for differentiable L0 estimation.
Modifications made:
- Initialization: Sample from scaled Bernoulli with fixed initial sparsity
  (instead of initializing all parameters to same value)
- Sparsity floor: Clip sparsity penalty at fixed minimum to prevent all 
  weights deactivating

Result: Performs consistently worse than TopK (higher loss at all sparsity levels).
See Figure 35.

10.2 ATTRIBUTION-BASED PRUNING
------------------------------
Select top nodes by gradient attribution to task loss.
Result: Drastically worse than learned pruning. See Figure 31.

================================================================================
SECTION 11: VALIDATION
================================================================================

11.1 CIRCUIT SUFFICIENCY
------------------------
Mean-ablating every neuron EXCEPT those in the circuit preserves task loss.

11.2 CIRCUIT NECESSITY ("Inverse Pruning")
------------------------------------------
Ablating ONLY the circuit nodes severely harms task loss.
See Figure 32.

11.3 CAUSAL SCRUBBING RELATIONSHIP
----------------------------------
Mean ablation is a weaker version of Causal Scrubbing condition 2.
Full Causal Scrubbing was found to be much harder to satisfy in early experiments.

Mean ablation advantages over activation patching:
- Activation patching misses circuit parts where activations differ across 
  prompt pairs but not within each pair
- Mean ablation leads to more complete circuits

================================================================================
SECTION 12: CIRCUIT EXAMPLES FROM PAPER
================================================================================

12.1 STRING CLOSING (single_double_quote)
-----------------------------------------
Circuit size: 12 nodes, 9 edges
Components: 2 MLP neurons, 1 QK channel, 1 V channel

Algorithm:
1. Layer 0 MLP combines (" and (' embeddings into:
   - "quote detector" neuron (channel 985, positive on both)
   - "quote type classifier" neuron (channel 460, positive on ", negative on ')
2. Layer 10 attention head 82 uses:
   - Quote detector as key (channel 1)
   - Quote type classifier as value (channel 0)
   - Constant positive query
3. Attention output correctly closes the string

Total edges connecting these 4 components to rest of network: 41
Edges used by circuit: 9

12.2 BRACKET COUNTING (bracket_counting)
----------------------------------------
Circuit size: 7 nodes, 4 edges (simplified)
Algorithm:
1. Embedding: [ token writes to residual channels 759, 826, 1711
2. Counting: Layer 2 attention head 125 channel 12 sums bracket detectors
   - Q ≈ 0, so attention is uniform average over context
   - Writes averaged "nesting depth" to residual channel 1249
3. Thresholding: Layer 4 attention head 80 uses attention sink
   - Uses nesting depth as query (channel 4)
   - Softmax acts as threshold
   - Outputs to residual channel 1079 ("nested list close") only on nested lists

Known failure mode: "context dilution" - fails on very long lists because 
activation is diluted by averaging.

12.3 VARIABLE TYPE TRACKING (set_or_string)
-------------------------------------------
Uses 4 QK channels and 3 V channels across 2 attention heads in successive layers.

Algorithm:
1. Layer 4 head 73 copies variable name "current" to set()/""token via value channel 8
2. Layer 6 head 26 uses variable name embedding as Q and K
3. Copies type info from set()/"" token to final token

================================================================================
SECTION 13: MODEL WIDTH SWEEP
================================================================================

From ablation figures, model widths tested: 256, 512, 1024, 2048

================================================================================
SECTION 14: ADDITIONAL NOTES
================================================================================

14.1 DEAD NEURONS
-----------------
Current optimization leads to many dead neurons, reducing efficiency.
Better reinitialization techniques could help.

14.2 APPROXIMATE TOP-K WARNING
------------------------------
Be careful with approximate top-k during training. In some experiments, using 
Key et al. (2024) caused optimization degradation because buckets lined up with 
matrix rows, inducing unintended structure.

14.3 SYSTEMS CONSIDERATIONS
---------------------------
Unstructured sparsity is unfriendly to modern GPUs:
- Breaks tiled dataflow that GEMM kernels rely on
- Reduces data reuse from on-chip SRAM
- Prevents use of Tensor Cores (which provide majority of throughput)

TopK kernel optimization:
- PyTorch's naive TopK uses radix sort (slow)
- Binary search over threshold to find k is much faster
- Allows parallelism across batch dimension for activation sparsity

14.4 MANUAL CIRCUIT CLEANUP
---------------------------
Pruning algorithm is imperfect and leaves small number of nodes with negligible 
impact on task loss. Additional manual pruning is sometimes needed for clean circuits.
Researcher spent roughly one researcher-day per circuit for manual interpretation.

================================================================================
END OF REPLICATION GUIDE
================================================================================

